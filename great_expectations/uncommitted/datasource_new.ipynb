{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7766b67b",
   "metadata": {},
   "source": [
    "# Create a new spark Datasource\n",
    "Use this notebook to configure a new spark Datasource and add it to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780bddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "from great_expectations.cli.datasource import sanitize_yaml_and_save_datasource, check_if_datasource_name_exists\n",
    "context = ge.get_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db996307",
   "metadata": {},
   "source": [
    "## Customize Your Datasource Configuration\n",
    "\n",
    "**If you are new to Great Expectations Datasources,** you should check out our [how-to documentation](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_datasources.html)\n",
    "\n",
    "**My configuration is not so simple - are there more advanced options?**\n",
    "Glad you asked! Datasources are versatile. Please see our [How To Guides](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_datasources.html)!\n",
    "\n",
    "Give your datasource a unique name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1538b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_name = \"bike_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae71256",
   "metadata": {},
   "source": [
    "### For files based Datasources:\n",
    "Here we are creating an example configuration.  The configuration contains an **InferredAssetFilesystemDataConnector** which will add a Data Asset for each file in the base directory you provided. It also contains a **RuntimeDataConnector** which can accept filepaths.   This is just an example, and you may customize this as you wish!\n",
    "\n",
    "Also, if you would like to learn more about the **DataConnectors** used in this configuration, including other methods to organize assets, handle multi-file assets, name assets based on parts of a filename, please see our docs on [InferredAssetDataConnectors](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_datasources/how_to_configure_an_inferredassetdataconnector.html) and [RuntimeDataConnectors](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/creating_batches/how_to_configure_a_runtime_data_connector.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37beca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: bike_data\n",
      "class_name: Datasource\n",
      "execution_engine:\n",
      "  class_name: SparkDFExecutionEngine\n",
      "data_connectors:\n",
      "  default_inferred_data_connector_name:\n",
      "    class_name: InferredAssetFilesystemDataConnector\n",
      "    base_directory: ../data\n",
      "    default_regex:\n",
      "      group_names: \n",
      "        - data_asset_name\n",
      "      pattern: (.*)\n",
      "  default_runtime_data_connector_name:\n",
      "    class_name: RuntimeDataConnector\n",
      "    batch_identifiers:\n",
      "      - default_identifier_name\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_yaml = f\"\"\"\n",
    "name: {datasource_name}\n",
    "class_name: Datasource\n",
    "execution_engine:\n",
    "  class_name: SparkDFExecutionEngine\n",
    "data_connectors:\n",
    "  default_inferred_data_connector_name:\n",
    "    class_name: InferredAssetFilesystemDataConnector\n",
    "    base_directory: ../data\n",
    "    default_regex:\n",
    "      group_names: \n",
    "        - data_asset_name\n",
    "      pattern: (.*)\n",
    "  default_runtime_data_connector_name:\n",
    "    class_name: RuntimeDataConnector\n",
    "    batch_identifiers:\n",
    "      - default_identifier_name\n",
    "\"\"\"\n",
    "print(example_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65246f",
   "metadata": {},
   "source": [
    "# Test Your Datasource Configuration\n",
    "Here we will test your Datasource configuration to make sure it is valid.\n",
    "\n",
    "This `test_yaml_config()` function is meant to enable fast dev loops. **If your\n",
    "configuration is correct, this cell will show you some snippets of the data\n",
    "assets in the data source.** You can continually edit your Datasource config\n",
    "yaml and re-run the cell to check until the new config is valid.\n",
    "\n",
    "If you instead wish to use python instead of yaml to configure your Datasource,\n",
    "you can use `context.add_datasource()` and specify all the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6249bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to instantiate class from config...\n",
      "\tInstantiating as a Datasource, since class_name is Datasource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/16 20:06:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSuccessfully instantiated Datasource\n",
      "\n",
      "\n",
      "ExecutionEngine class name: SparkDFExecutionEngine\n",
      "Data Connectors:\n",
      "\tdefault_inferred_data_connector_name : InferredAssetFilesystemDataConnector\n",
      "\n",
      "\tAvailable data_asset_names (2 of 2):\n",
      "\t\t202201-divvy-tripdata.csv (1 of 1): ['202201-divvy-tripdata.csv']\n",
      "\t\t202202-divvy-tripdata.csv (1 of 1): ['202202-divvy-tripdata.csv']\n",
      "\n",
      "\tUnmatched data_references (0 of 0):[]\n",
      "\n",
      "\tdefault_runtime_data_connector_name:RuntimeDataConnector\n",
      "\n",
      "\tAvailable data_asset_names (0 of 0):\n",
      "\t\tNote : RuntimeDataConnector will not have data_asset_names until they are passed in through RuntimeBatchRequest\n",
      "\n",
      "\tUnmatched data_references (0 of 0): []\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<great_expectations.datasource.new_datasource.Datasource at 0x168cd0e20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.test_yaml_config(yaml_config=example_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c1953",
   "metadata": {},
   "source": [
    "## Save Your Datasource Configuration\n",
    "Here we will save your Datasource in your Data Context once you are satisfied with the configuration. Note that `overwrite_existing` defaults to False, but you may change it to True if you wish to overwrite. Please note that if you wish to include comments you must add them directly to your `great_expectations.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be637a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'module_name': 'great_expectations.datasource',\n",
       "  'execution_engine': {'class_name': 'SparkDFExecutionEngine',\n",
       "   'module_name': 'great_expectations.execution_engine'},\n",
       "  'data_connectors': {'default_inferred_data_connector_name': {'module_name': 'great_expectations.datasource.data_connector',\n",
       "    'base_directory': '../data',\n",
       "    'class_name': 'InferredAssetFilesystemDataConnector',\n",
       "    'default_regex': {'group_names': ['data_asset_name'], 'pattern': '(.*)'}},\n",
       "   'default_runtime_data_connector_name': {'module_name': 'great_expectations.datasource.data_connector',\n",
       "    'class_name': 'RuntimeDataConnector',\n",
       "    'batch_identifiers': ['default_identifier_name']}},\n",
       "  'class_name': 'Datasource',\n",
       "  'name': 'bike_data'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanitize_yaml_and_save_datasource(context, example_yaml, overwrite_existing=False)\n",
    "context.list_datasources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8478256d",
   "metadata": {},
   "source": [
    "Now you can close this notebook and delete it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cea390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
